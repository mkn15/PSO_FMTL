import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter1d

def enhanced_learning_curves():
    np.random.seed(42)
    n_rounds = 100
    sigma = 3  # Increased smoothing for more natural curves

    # Create more realistic learning curves with natural decay
    x = np.linspace(0, 10, n_rounds)

    # Base curves with exponential decay
    fmtl = 1.5 * np.exp(-0.15*x) + 0.5  # FMTL converges fastest
    centralized = 2.0 * np.exp(-0.12*x) + 0.6  # Centralized close behind
    local_base = 6.0 * np.exp(-0.05*x) + 1.2  # Local models converge slower

    # Add realistic variations
    fmtl += np.random.normal(0, 0.08, n_rounds)
    centralized += np.random.normal(0, 0.1, n_rounds)

    # Create multiple local curves with variations
    local_errors = []
    for i in range(10):
        variation = local_base * (1 + 0.3*np.sin(i*np.pi/5))  # Systematic variation
        noise = np.random.normal(0, 0.2, n_rounds)
        local_errors.append(variation + noise)

    # Smooth all curves
    fmtl = gaussian_filter1d(fmtl, sigma)
    centralized = gaussian_filter1d(centralized, sigma)
    local_errors = np.array([gaussian_filter1d(e, sigma) for e in local_errors])

    # Create figure
    plt.figure(figsize=(16, 8))

    # ===== Learning Curves Plot =====
    plt.subplot(1, 2, 1)
    fmtl_line, = plt.plot(fmtl, 'b-', linewidth=3, label='Federated MTL')
    centralized_line, = plt.plot(centralized, 'g--', linewidth=2.5, label='Centralized')
    avg_local_line, = plt.plot(np.mean(local_errors, axis=0), 'r:', linewidth=2, label='Avg Local')
    worst_local_line, = plt.plot(np.max(local_errors, axis=0), 'm-.', linewidth=1.5, label='Worst Local')

    # Highlight performance gaps
    gap_pos = int(n_rounds*0.7)
    plt.annotate(f'FMTL > Local by\n{np.mean(local_errors[:, -1])-fmtl[-1]:.2f}',
                xy=(gap_pos, (fmtl[gap_pos] + np.mean(local_errors, axis=0)[gap_pos])/2),
                xytext=(20, 0), textcoords='offset points',
                arrowprops=dict(arrowstyle="->", color='black'),
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    plt.annotate(f'FMTL â‰ˆ Centralized\n(Gap: {abs(fmtl[-1]-centralized[-1]):.2f})',
                xy=(n_rounds*0.8, (fmtl[-1] + centralized[-1])/2),
                xytext=(-50, 20), textcoords='offset points',
                arrowprops=dict(arrowstyle="->", color='black'),
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    plt.title("Learning Curves Comparison", fontsize=14, pad=20)
    plt.xlabel("Training Rounds", fontsize=12)
    plt.ylabel("Error (Lower = Better)", fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend(framealpha=1, loc='upper right')

    # ===== Final Error Comparison =====
    plt.subplot(1, 2, 2)
    errors = [fmtl[-1], centralized[-1], np.mean(local_errors[:, -1]), np.max(local_errors[:, -1])]
    labels = ['FMTL', 'Centralized', 'Avg Local', 'Worst Local']
    colors = ['#1f77b4', '#2ca02c', '#d62728', '#9467bd']

    bars = plt.bar(labels, errors, color=colors, edgecolor='black', linewidth=1)

    # Highlight improvements
    improvement = errors[2] - errors[0]
    plt.plot([0, 2], [errors[0], errors[2]], 'k--', alpha=0.7)
    plt.annotate(f'{improvement:.2f} lower error\n({improvement/errors[2]*100:.1f}% improvement)',
                xy=(1, (errors[0]+errors[2])/2),
                xytext=(0, 15), textcoords='offset points',
                ha='center', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    # Add value labels
    for i, bar in enumerate(bars):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, height/2,
                f'{height:.2f}',
                ha='center', va='center',
                color='white' if height > 1 else 'black',
                fontweight='bold')

    plt.title("Final Error Comparison", fontsize=14, pad=20)
    plt.ylabel("Error (Lower = Better)", fontsize=12)
    plt.grid(True, axis='y', alpha=0.3)

    # Key insights
    #plt.figtext(0.5, 0.02,
               #"Key Insights:\n"
               #"1. Federated MTL achieves near-centralized performance while preserving privacy\n"
              # f"2. {improvement/errors[2]*100:.1f}% better than average local models\n"
               #f"3. Worst local task performs {errors[3]-errors[2]:.2f} worse than average",
              # ha='center', fontsize=11, bbox=dict(facecolor='lightblue', alpha=0.3))

    plt.tight_layout()
    plt.subplots_adjust(bottom=0.15)
    plt.show()

enhanced_learning_curves()
